<br><!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Research Portfolio</title>
<link rel="stylesheet" type="text/css" href="css/styles.css">
<link rel="stylesheet" type="text/css" href="css/styles-class.css">
</head>

<body class="research-portfolio">

<!---->
<section>
  <div>
  </div>
  <div>
    <nav class="desktopOnly" style="padding-bottom: 20px;"> <a href="index.html" style="color: magenta; text-decoration: none;">Practical&nbsp;Resolution</a> &bull; <a href="#" style="color: magenta; text-decoration: none;">Critical&nbsp;Rationale</a> &bull; <a href="#" style="color: magenta; text-decoration: none;">Critical&nbsp;Context&nbsp;Paper</a> &bull; <a href="#" style="color: magenta; text-decoration: none;" class="active">Research&nbsp;Portfolio</a> </nav>
    <h1 style="color: magenta" class="h1ToH2OnMobile">Research Portfolio</h1>
  </div>
</section>

<!---->
<section>
  <div>
    <p class="left-title"> About this page </p>
  </div>
  <div>
    <p> What is documented in the research portfolio?</p>
    Relevant supporting developmental materials of Glitch Room &horbar; my MA Graphic Media Design major project at London College of Communication, University of the Arts London (UAL) in 2019.<br>
    <p>Why is it presented as a webpage? </p>
    1 To be cohesive with other parts of the project.<br>
    2 To enable live demonstration of past experiments inside this document, as some are only executable within a browser.<br>
    <p>Why is it mainly constructed by questions and answers?</p>
    1 For myself to be reflective by answering the questions.<br>
    2 To make it more engaging for the reader by raising curiosity with questions.<br>
    3 To make the contents easy to navigate through.<br>
  </div>
</section>

<!---->
<section>
  <div>
    <p class="left-title">Pattern as subject</p>
  </div>
  <div>
    <p>Why was pattern chosen as my research subject?</p>
    To fulfil my wish of exploring more visual-oriented areas rather than social-related topics.<br>
    <p>What experiments did I do?</p>
    1 The Letter Evolution: an experiment which aims at exploring the usage and effect of pattern making techniques on typographic design.<br>
    <div class="grid3">
      <img src="img/the-letter-evolution-2.jpg" alt="">
      <img src="img/the-letter-evolution-3.jpg" alt="">
      <img src="img/the-letter-evolution-4.jpg" alt="">
    </div>
    <h6>&#8679; Example spreads of ‘The Letter Evolution’ by Nora Zhao (2019).</h6>
    2 Emotional Words: an experiment which explores the meaning and typographic pattern by highlighting and isolating enclosed space in written words.<br>
    <div class="grid2">
      <img src="img/emotional-words-1.jpg" alt="">
      <img src="img/emotional-words-2.jpg" alt="">
    </div>
    <h6>&#8679; Cover and an example spread of ‘Emotional Words’ by Nora Zhao (2019).</h6>
    3 Google Translate: an experiment which aims to visualise the influence of machine translation by showing a piece of sample text.<br>
    <div class="grid4">
      <img src="img/google-translate-1.jpg" alt="">
      <img src="img/google-translate-2.jpg" alt="">
      <img src="img/google-translate-3.jpg" alt="">
      <img src="img/google-translate-4.jpg" alt="">
    </div>
    <h6>&#8679; Cover and example spreads of ‘Google Translate’ by Nora Zhao (2019).</h6>
    <p>How do I evaluate these experiments? </p>
    1 Strength: explorations of pattern from different aspects.<br>
    2 Weakness: limited depth of investigation on pattern, and lack of iterations due to frequent change of directions.<br>
    <p>How to improve it? </p>
    To zoom down area of intended investigation and find a focus.<br>
  </div>
</section>

<!---->
<section>
  <div>
    <p class="left-title">Unique pattern as focus</p>
  </div>
  <div>
    <p>What was chosen as the focus?</p>
    Mass-produced unique pattern.<br>
    <p>What secondary research informed this decision?</p>
    Creative Review Annual 2019: each copy comes with a unique cover design created by Alex Trochut (<a href="https://www.creativereview.co.uk/one-of-a-kind-how-our-unique-annual-covers-were-created/">more info</a>).<br>
    <div class="grid4">
      <img src="img/creative-review-cover-1.jpg" alt="">
      <img src="img/creative-review-cover-2.jpg" alt="">
      <img src="img/creative-review-cover-3.jpg" alt="">
      <img src="img/creative-review-cover-4.jpg" alt="">
    </div>
    <h6>&#8679; Selected covers of 'Creative Review Annual 2019' by Alex Trochut (2019).</h6>
    The appeal of this project is that it doesn’t aim at making one specific design. Instead, by making use of a generative tool, the designer was able to create unique design pieces. The results, as well as the approach, is stimulating.<br>
    <p> What was used as the generator of these variations? </p>
    HP Mosaic: a software which takes vector-based seed files as input and generates a set number of different output by random scaling, trans-positioning and rotating.<br>
    <div class="grid1">
      <img src="img/hp-mosaic.png" alt="">
    </div>
    <h6>&#8679; Illustrated mechanism of 'HP Mosaic' by HP (2018). </h6>
    <p>What are the limitations of this software?</p>
    1 The options offered are limited.<br>
    2 The results, directly extracted from the already made designs, are more or less expected.<br>
  </div>
</section>

<!---->
<section>
  <div>
    <p class="left-title">Tool and method</p>
  </div>
  <div>
    <p>What tool may be adopted for my project?</p>
    Coding: through writing code manually, I may build an executable program in which underlying computing tasks are constructed from my specific design needs.<br>
    <p>Why?</p>
    1 To break the restrictions set by ready-made design software.<br>
    2 To work directly with the medium which constructs digital design output.<br>
    3 Most importantly, to learn to code through practice and develop it as a skill to improve my competitiveness in the future.<br>
    <p> What experiment did I do? </p>
    Mouse XY: an experiment in which the construction of pattern is made viable through mouse movement. Hover over your mouse to draw. Double click to reset current canvas. Right click to save current image.<br>
    P5.js JavsScript library is used to create the canvases (<a href="https://p5js.org/">more info</a>).<br>
    <div class="grid2" style="grid-gap: 8px;">
      <div style="position:relative; padding-top:100%; overflow:auto; border:1px solid white;">
        <iframe src="experiment/p5-1.html" style="border:none; position:absolute; top:0; left:0; height:100%; width:100%;"> </iframe>
      </div>
      <div style="position:relative; padding-top:100%; overflow:auto; border:1px solid white;">
        <iframe src="experiment/p5-2.html" style="border:none; position:absolute; top:0; left:0; height:100%; width:100%;"> </iframe>
      </div>
    </div>
    <h6>&#8679; Example canvases of 'Mouse XY' by Nora Zhao (2019). </h6>
    <div class="grid4">
      <img src="img/mouse-xy-1.jpg" alt="">
      <img src="img/mouse-xy-2.jpg" alt="">
      <img src="img/mouse-xy-3.jpg" alt="">
      <img src="img/mouse-xy-4.jpg" alt="">
    </div>
    <h6>&#8679; Example image output of 'Mouse XY' by Nora Zhao (2019). </h6>
    <p>How do I evaluate this experiment?</p>
    1 Strength: it’s responsive and playful.<br>
    2 Weakness: it’s highly reliant on manual input. Therefore, output quality may vary depending on the skill of the ‘painter’ and effort put in ‘painting’ the picture.<br>
    <p> How to improve it? </p>
    To build a program which functions automatically (generative).<br>
    <p>What demonstrated its viability?</p>
    Distinction Machine: a project by Kim Albrecht which explores computational behaviour under ambiguous situations. Geometries of different colours are positioned at the same location to produce pattern (<a href="https://distinctionmachine.kimalbrecht.com/">more info</a>).<br>
    <div class="grid1">
      <img src="img/distinction-machine.jpg" alt="">
    </div>
    <h6>&#8679; Models of 'Distinction Machine' by Kim Albrecht (2019). </h6>
    <p>What technique was used to generate the dynamic output?</p>
    Z-fighting: a phenomenon in 3D rendering which occurs when two or more layers are positioned in identical or proximate positions. The colours ‘fight’ with each other to be shown on screen pixels and result in a flickering rasterisation effect.<br>
  </div>
</section>

<!---->
<section>
  <div>
    <p class="left-title"> Construction and investigation</p>
  </div>
  <div>
    <p>What experiment did I do? </p>
    Fighting Pattern: an experiment which generates pattern from a scene of z-fighting, in which the most straightforward geometric surface, flat plane, is used.<br>
    Three.js is used to create the scenes. It is a JavaScript library for creating and displaying animated 3D computer graphics in web browsers (<a href="https://threejs.org/">more info</a>).<br>
    <div class="grid4">
      <div class="image-container">
        <img src="img/fighting-pattern-1.png" alt="">
        <div class="top-left-text">
          <h6>Front facing (1)</h6>
        </div>
      </div>
      <div class="image-container">
        <img src="img/fighting-pattern-2.png" alt="">
        <div class="top-left-text">
          <h6>Front facing (2)</h6>
        </div>
      </div>
      <div class="image-container">
        <img src="img/fighting-pattern-3.png" alt="">
        <div class="top-left-text">
          <h6>Tilted angle (1)</h6>
        </div>
      </div>
      <div class="image-container">
        <img src="img/fighting-pattern-4.png" alt="">
        <div class="top-left-text">
          <h6>Tilted angle (2)</h6>
        </div>
      </div>
    </div>
    <h6>&#8679; Example output of 'Fighting Pattern' by Nora Zhao (2019).</h6>
    <div class="grid1">
      <img src="img/fighting-pattern-framework.png" alt="">
    </div>
    <h6>&#8679; Framework diagram of 'Fighting Pattern' by Nora Zhao (2019).</h6>
    <p>What did I notice? </p>
    Generated images may vary while the planes are tilted to different angles.<br>
    <p>What to be done next?</p>
    An investigation of z-fighting behaviour with the method of controlled experiments in which everything is held constant except for one variable.<br>
    <p>What are the findings?</p>
    Changes of camera position and plane rotation angle on x and y axes may result in different appearances of generated images.<br>
    <div class="grid3">
      <div class="image-container">
        <img src="img/z-fighting-investigation-1.png" alt="">
        <div class="top-left-text">
          <h6>Camera position on z axis: 1</h6>
        </div>
      </div>
      <div class="image-container">
        <img src="img/z-fighting-investigation-2.png" alt="">
        <div class="top-left-text">
          <h6>Camera position on z axis: 0.8</h6>
        </div>
      </div>
      <div class="image-container">
        <img src="img/z-fighting-investigation-3.png" alt="">
        <div class="top-left-text">
          <h6>Camera position on z axis: 0.2</h6>
        </div>
      </div>
    </div>
    <h6>&#8679; Example test results of camera positions on z axis by Nora Zhao (2019).</h6>
    <div class="grid3">
      <div class="image-container">
        <img src="img/z-fighting-investigation-4.png" alt="">
        <div class="top-left-text">
          <h6>Plane rotation: on both x and y axes</h6>
        </div>
      </div>
      <div class="image-container">
        <img src="img/z-fighting-investigation-5.png" alt="">
        <div class="top-left-text">
          <h6>Plane rotation: on x axis only</h6>
        </div>
      </div>
      <div class="image-container">
        <img src="img/z-fighting-investigation-6.png" alt="">
        <div class="top-left-text">
          <h6>Plane rotation: on y axis only</h6>
        </div>
      </div>
    </div>
    <h6>&#8679; Example test results of plane rotation combinations on x and y axes by Nora Zhao (2019).</h6>
    <div class="grid3">
      <div class="image-container">
        <img src="img/z-fighting-investigation-7.png" alt="">
        <div class="top-left-text">
          <h6>Plane rotation on x axis: 0.09</h6>
        </div>
      </div>
      <div class="image-container">
        <img src="img/z-fighting-investigation-8.png" alt="">
        <div class="top-left-text">
          <h6>Plane rotation on x axis: 0.54</h6>
        </div>
      </div>
      <div class="image-container">
        <img src="img/z-fighting-investigation-9.png" alt="">
        <div class="top-left-text">
          <h6>Plane rotation on x axis: 0.74</h6>
        </div>
      </div>
    </div>
    <h6>&#8679; Example test results of plane rotations on x axis by Nora Zhao (2019). Similar situations occur with rotation on y axis. </h6>
  </div>
</section>

<!---->
<section>
  <div>
    <p class="left-title">Interaction integration</p>
  </div>
  <div>
    <p> How to make the findings accessible? </p>
    To add control sliders for these variables to allow results of different looks to be generated.<br>
    <p>What other features can be added? </p>
    1 Colour adjustments option: to allow setting up colours of personal preferences;<br>
    2 Random appearances: to create varied results upon each visit (or refreshment of the page) by combining randomly generated colour, plane rotation and camera position values;<br>
    3 Screenshot function: to offer a way of preserving generated results.<br>
    <p>What is the result of this experiment?</p>
    Configurable Pattern: a canvas in which an interactive control panel is implemented for customisable generation of z-fighting pattern. Use the control panel or press refresh button to generate new looks.<br>
    <div class="grid1">
      <div style="position:relative; padding-top:70%; overflow:auto; border:1px solid white;">
        <iframe src="experiment/pattern-3.html"> </iframe>
      </div>
    </div>
    <h6>&#8679; 'Configurable Pattern' by Nora Zhao (2019).</h6>
    <div class="grid5" style="grid-gap: 8px;">
      <div style="position:relative; padding-top:100%; overflow:auto; border:1px solid white;">
        <iframe src="experiment/pattern-2.html"> </iframe>
      </div>
      <div style="position:relative; padding-top:100%; overflow:auto; border:1px solid white;">
        <iframe src="experiment/pattern-2.html"> </iframe>
      </div>
      <div style="position:relative; padding-top:100%; overflow:auto; border:1px solid white;">
        <iframe src="experiment/pattern-2.html"> </iframe>
      </div>
      <div style="position:relative; padding-top:100%; overflow:auto; border:1px solid white;">
        <iframe src="experiment/pattern-2.html"> </iframe>
      </div>
      <div style="position:relative; padding-top:100%; overflow:auto; border:1px solid white;">
        <iframe src="experiment/pattern-2.html"> </iframe>
      </div>
    </div>
    <h6>&#8679; Side by side display of 'Configurable Pattern' by Nora Zhao (2019).</h6>
    <p>Is there another variation of this experiment?</p>
    Yes. To add a focal point in the canvas, I also created a version with a circle in the middle. The colour, size, and location of the circle can be adjusted through the control panel, or be completely dismissed from the canvas by setting its scale to 0.<br>
    Additionally, from generated images, postcards were printed to materialise the artefacts.
    <div class="grid2" style="grid-row-gap: 8px;">
      <div style="position:relative; padding-top:50%; overflow:auto; border:1px solid white;">
        <iframe src="experiment/dot.html"> </iframe>
      </div>
      <img src="img/drop-the-dot-postcard.jpg" alt="">
    </div>
    <h6>&#8679; Live demonstration and printed postcards of 'Drop the Dot' by Nora Zhao (2019).</h6>
  </div>
</section>

<!---->
<section>
  <div>
    <p class="left-title">Sensor integration</p>
  </div>
  <div>
    <p>What can be explored further?</p>
    More organic ways of interaction.<br>
    <p>What secondary research facilitated this thought?</p>
    At ‘The Shift’ exhibition, one of Rafaël Rozendaal’s websites (<a href="http://www.nosquito.biz/">source</a>) was transformed into an installation: when the visitor moves in the room, the sound of a mosquito will be played. Otherwise, the place stays silent (<a href="https://www.youtube.com/watch?v=6Zx1lRdhego">more info</a>).<br>
    <div class="grid3">
      <img src="img/mosquito-1.jpg" alt="">
      <img src="img/mosquito-2.jpg" alt="">
      <video width="100%" controls  style="border:1px solid white;">
        <source src="img/mosquito-3.mp4" type="video/mp4">
        Your browser does not support the video tag. </video>
    </div>
    <h6>&#8679; ‘The Shift’ solo show at W139 Amsterdam by Rafaël Rozendaal (2011). </h6>
    This straightforward but playful example demonstrates that the interaction between the user and artwork can be responsive and organic.<br>
    Currently, my project only allows users to adjust the variables manually. However, ways of interaction can be further explored. In spite that a digital device is used to generated and presented the artwork, there can still be ways of bringing in external factors.<br>
    <p>How?</p>
    To reconsider the system’s input methods and parameters.<br>
    To be specific, environment-related factors, such as sound, light and movement can be turned into data by sensors and be utilised as automatic and continuous input of the image generating system.<br>
    <p>How to collect data?</p>
    From 'Plant-to-Plant Protocol' (a Supra Systems Studio workshop), I studied the basics of data transmission within the scope of physical computing in practice. This experience has informed me of the viability of using physical computing to collect and transport data from sensors to my image generation system.<br>
    <div class="flex">
      <div style="flex: calc(2000/2828);">
        <img src="img/plant-to-plant-protocol-1.jpg" alt="">
      </div>
      <div style="flex: calc(2016/1352);">
        <img src="img/plant-to-plant-protocol-5.jpg" alt="">
      </div>
      <div style="flex: calc(1430/948);">
        <img src="img/plant-to-plant-protocol-6.jpg" alt="">
      </div>
      <div style="flex: calc(1080/1080);">
        <img src="img/plant-to-plant-protocol-3.jpg" alt="">
      </div>
      <div style="flex: calc(1500/2000);">
        <img src="img/plant-to-plant-protocol-7.jpg" alt="">
      </div>
    </div>
    <h6>&#8679; Poster, the workshop and field device installation of 'Plant-to-Plant Protocol' (2019).</h6>
    With further research and technical support from the creative technology lab in LCC, I managed to connect the sound and light sensor to my model for data input.<br>
    <p>How to use the data as input of my experiment?</p>
    Since data collected by the sensors are mathematical values, it becomes a matter of using the numbers to influence components of the model:<br>
    1 Light: to change the brightness of colours because of their strong visual associations.<br>
    2 Sound: to control the scale of the model, so that the model has an approximate effect of sound visualisation and becomes a bridge which links hearing and sight senses.<br>
    <p>Why is sphere used in this experiment?</p>
    1 To add a sense of depth in space.<br>
    2 for extended flexibility: to have rotation along all x, y and z axes.<br>
    3 To create distinctive pattern compared to the previous experiment (due to sphere’s unique structure).<br>
    <p>What else can be added in the system?</p>
    Gradual colour change: to enhance the dynamics of the generated visual results in such a way even when the data input doesn't trigger noticeable variations of the model, new colour combinations are still being formulated every moment.<br>
    <p>What is the result of this experiment?</p>
    Reactive Sphere: an automatically functioning responsive sphere model which produces bespoke pattern according to the combination of inner and outer factors.<br>
    <div class="grid3">
      <img src="img/reactive-sphere-1.jpg" alt="">
      <img src="img/reactive-sphere-4.jpg" alt="">
      <img src="img/reactive-sphere-2.jpg" alt="">
    </div>
    <h6>&#8679; Components sensors and installation view of 'Reactive Sphere' in 'GMD Work in Progress' show by Nora Zhao (2019). </h6>
    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx sol lewitt
    <div class="flex">
      <div style="flex: calc(1248/1403);">
        <img src="img/wall-drawing-1.png" alt="">
      </div>
      <div style="flex: calc(1847/1270);">
        <img src="img/wall-drawing-2.png" alt="">
      </div>
    </div>
    <h6>&#8679; Installation view of 'Focus: Sol LeWitt' exhibition: 'Wall Drawing #260 (1975)'. Photograph by Thomas Griesel (2008-2009). </h6>
  </div>
</section>

<!---->
<section>
  <div>
    <p class="left-title">Database integration</p>
  </div>
  <div>
    <p>Is there any other data suitable for my project?</p>
    Zooming out of the environment near us and thinking bigger, there are also 'activities' of the mother nature that can be used as numeric input.<br>
    <p>What secondary research facilitated this idea?</p>
    To reflect global diversity, D&AD's 2013 Annual cover, designed by Fleur Isbell, displayed 196 code generated illustrations from meta and location data for each country in the world (<a href=" https://fleur.isbell.net/portfolio/page01.html ">more info</a>).<br>
    <div class="grid3">
      <img src="img/d&ad-cover-1.jpg" alt="">
      <img src="img/d&ad-cover-2.jpg" alt="">
      <img src="img/d&ad-cover-3.jpg" alt="">
    </div>
    <h6>&#8679; 'D&AD Annual 2013 – Global Horizons' by Fleur Isbell (2013).</h6>
    This project demonstrated a viable way of using location-related data for automated visual generation. Similarly, weather data can be also be utilised in my project as it is not only dynamic but also relevant to our daily life.<br>
    <p>Where can I obtain the data? </p>
    Online database: for my project, the weather data is fetched from 'OpenWeather' (<a href=" https://openweathermap.org/">more info</a>).<br>
    <p>What is the result of this experiment?</p>
    City Sphere: a system which takes live weather data of designated city and generates corresponding model upon request.<br>
    <div class="grid1">
      <div style="position:relative; padding-top:70%; overflow:auto; border:1px solid white;">
        <iframe src="experiment/city-sphere.html"
                style="height:0; max-height:100%; min-height:100%; width:0; max-width:100%; min-width:100%;"> </iframe>
      </div>
    </div>
    <h6>&#8679; 'City Sphere' by Nora Zhao (2019).</h6>
    <div class="grid1">
      <img src="img/city-sphere.png" alt="">
    </div>
    <h6>&#8679; Output stills of 'City Sphere' at 18:35, 07 November 2019. Image by Nora Zhao (2019).</h6>
    <p>What can be further explored?</p>
    If data from different cities are applied to the system, a series of results can be generated and presented  simultaneously. The form of this experiment may be altered for installation.<br>
    <p>What secondary research encouraged this idea?</p>
    Prismatica: an installation created by RAW Design to engage visitors and invoke a feeling or mood from day to night. It consists of 50 prisms that glimmer under natural light by day and provide atmospheric lighting by night (<a href="https://www.rawdesign.ca/projects/prismatica/">more info</a>).<br>
    <div class="grid2">
      <img src="img/prismatica-installation-1.jpg" alt="">
      <img src="img/prismatica-installation-2.jpg" alt="">
    </div>
    <h6>&#8679; 'Prismatica' by RAW Design (2015).</h6>
    This project differs from mine because the natural lighting, rather than live data, causes changes here. However, its quantitative approach and tangibility of physical objects are worth considering.<br>
    Specifically, it can be a number of spheres hanging in an exhibition room and have the generated moving image shown on their surfaces with the method of projection mapping. In this way, a physical space is created where various dynamic 3-dimensional artefacts can be viewed simultaneously.<br>
  </div>
</section>

<!---->
<section>
  <div>
    <p class="left-title">Review and preview</p>
  </div>
  <div>
    <p>What have I explored so far?</p>
    From the last three experiments, I explored input parameters respectively from the following aspects:<br>
    1 Configurable Pattern: manual data input from the users according to their preferences.<br>
    2 Reactive Sphere: automatic data input from sensors which collect information its surrounding environment.<br>
    3 City Sphere: data input from a live database. The information sits within a broader scope and is collected from corresponding professional agencies. After setting the target object (a city), data will be automatically fetched and updated to the system.<br>
    <p>What is the commonality of these experiments?</p>
    The concept of co-creation is integrated by having the system set up with three factors which influence the outcome.<br>
    Following is the illustrated framework of ‘Reactive Sphere’ which demonstrates its mechanism.<br>
    <div class="grid1">
      <img src="img/reactive-sphere-framework.png" alt="">
    </div>
    <h6>&#8679; Illustrated framework of ‘Reactive Sphere' by Nora Zhao (2019).</h6>
    <p>What are the differences in these experiments?</p>
    Customisable options are available in the first and last experiment, whereas the second experiment produces personalised results.<br>
    1 Configurable Pattern: customisation through the control panel.<br>
    2 Reactive Sphere: personalised result by generating a model that can be influenced by its viewers’ activities.<br>
    3 City Sphere: customisation by allowing users to connect the model with chosen cities.<br>
  </div>
</section>

<!---->
<section>
  <div>
    <p class="left-title">Design choices</p>
  </div>
  <div>
    <p>Why do I choose black as background colour of the webpages?</p>
    1 To visually exhibit the difference of screen as medium as opposed to paper as medium. Black is rarely used as background colour in print because it usually requires special ink. But in digital artefacts, the use of black is rather quite common without added cost.<br>
    2 Black as the darkest colour guarantees colourful models to stand out and be the centre of the viewers’ focus.<br>
    <p>Why are vivid colours chosen to be used?</p>
    Glitch art is always strongly associated with vivid colours visually. Take the example below where glitch artist Antonio Roberts transformed the binary data of audio files into images with intense and bright colours (<a href="https://www.hellocatfood.com/#some-of-my-favourite-songs">more info</a>).<br>
    <div class="grid3">
      <img src="img/some-of-my-favourite-songs-1.png" alt="">
      <img src="img/some-of-my-favourite-songs-2.png" alt="">
      <img src="img/some-of-my-favourite-songs-3.png" alt="">
    </div>
    <h6>&#8679; ‘Some of My Favourite Songs’ by Antonio Roberts (2012).</h6>
    I want my project to evoke the sense of glitch not only form the flickering effects, but also the colours it exhibits. As the medium of exhibiting digital artwork, screen uses RGB colour model in which red, green and blue light are added together in various ways to reproduce a broad array of colours.<br>
    <div class="flex">
      <div style="flex: calc(1/1);">
        <img src="img/rgb-1.png" alt="">
      </div>
      <div style="flex: calc(590/266)">
        <img src="img/rgb-2.jpg" alt="">
      </div>
    </div>
    <h6>&#8679; Left: demonstration of the principle of additive colour mixing
      (<a href="https://en.wikipedia.org/wiki/File:AdditiveColor.svg">source</a>). &#8679; Right: RGB sub-pixels in an LCD TV. An orange and a blue colour on the right, and a close-up shot on the left (<a href="https://en.wikipedia.org/wiki/RGB_color_model#/media/File:RGB_pixels.jpg">source</a>).</h6>
    Purposefully, I assigned red, green and blue as the initial colours of the models. Afterwards, when other colours are created and applied to the model, it reflects the additive colour model in a sense that all other colours are created from a mixture of the primary colours of red, green and blue.<br>
    Additionally, magenta is used in text and other elements to ensure readability when they overlap with the models.<br>
    <p>Why are the models self-rotating?</p>
    ‘Repositions (Double Blue)’ is an investigation into the performative nature of software by Jan Robert Leegte (<a href="http://www.repositionsblue.work/">source</a>). Contrary to freezing or capturing time, digital media are closest related to the performance arts, where the act lives in the now. Using random live algorithms, the documents are tirelessly striking new poses, together entangled in an indefinite choreography of repositions.<br>
    <div class="grid3">
      <img src="img/double-blue-1.png" alt="">
      <img src="img/double-blue-2.png" alt="">
      <img src="img/double-blue-3.png" alt="">
    </div>
    <h6>&#8679; Screenshots of ‘Repositions (Double Blue)’ by Jan Robert Leegte (2018).</h6>
    Rather than making a model staying still on the screen, which is similar to how an artwork is being exhibited in the museum, I adopted the the same approach as shown in Leegte’s work by making them rotate on its own.<br>
  </div>
</section>

<!---->
<section>
  <div>
    <p class="left-title">Outcome development</p>
  </div>
  <div>
    <p>Why isn’t the project completely finished as of now?</p>
    Due to time limit and technical difficulties, many planned functions aren’t implemented in project practical resolution yet.<br>
    Even though the current website works as it is, adding these functions may significantly improve its appeal and interactivity.<br>
  </div>
</section>

<!---->
<section>
  <div>
    <p class="left-title">Future application</p>
  </div>
  <div>
    <p>Home environment projection</p>
    ‘Universe of Water Particles, Transcending Boundaries’ is an interactive digital installation in which the flow of water transforms due to the interaction of people (<a href="https://www.teamlab.art/ew/asm_waterparticles-transcending/">more info</a>).<br>
    <div class="grid2">
      <img src="img/universe-of-water-particles-1.jpeg" alt="">
      <img src="img/universe-of-water-particles-2.jpeg" alt="">
    </div>
    <h6>&#8679; ‘Universe of Water Particles, Transcending Boundaries’ by teamLab (2017).</h6>
    What interests me is that it uses projection to alter the surroundings. Applying the same method, personalised and customised pattern can also be projected to walls at home, creating a unique and dynamic environment.<br>
    For instance, it may reflect the user’s moods by wirelessly connecting the image generation system with a smart device such as an Apple Watch for biological data collection in real-time. Ultimately, it establishes another form of self-expression.<br>
    <p>Live corporate identity</p>
    For the city of Melbourne’s rebranding project by Landor in 2010, the diversity of Melbourne is celebrated in its visual identity through colour, forms, facets, and structures.<br>
    The logo sets introduced me the concept of dynamic identity, which is quite fascinating compared to the conventional one logo approach. However, while providing great variety in logos, I think that there may be a qualitative approach that reinforces the link between underlying meaning and its visual representation rather than a solely quantitive approach (<a href="https://www.behance.net/gallery/276451/City-of-Melbourne">more info</a>).<br>
    <div class="flex">
      <div style="flex: calc(600/590);">
        <img src="img/corporate-identity-melbourne-2.jpg" alt="">
      </div>
      <div style="flex: calc(1430/784);">
        <img src="img/corporate-identity-nordkyn.gif" alt="">
      </div>
    </div>
    <h6> &#8679; Left: different versions of City of Melbourne's new logo by Landor (2009). &#8679; Right: logo for the Nordkyn tourism website by Neue Design Studio (2010). </h6>
    On the Nordkyn tourism website, its logo updates every five minutes based on weather data from the Norwegian Meteorological Institute. The Arctic climate and dramatic weather conditions become part of the visual identity itself (<a href="https://medium.com/@pdtv/dynamic-identities-85abb28fef2c">Source</a>). <br>
  </div>
</section>
</body>
</html>
